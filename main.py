import connect
from feature import get_net, show_net
# from getFeatures import get_f1,get_f2,get_f3
import networkx
import pandas as pd
from sklearn.model_selection import train_test_split
from Learning import trainall
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import r2_score
from sklearn.metrics import confusion_matrix

net = get_net("pickleX77.p")

def get_f1(user):
    return list(net.neighbors(user))


def get_f3(users): # G.out_degree(1) average
    sum = 0
    for usr in users:
        sum += (net.in_degree(usr))
    return sum/len(users)    


#positive_users = [48424,43169,49287,43489,35322,47991,43450,43499,35351,47764,35323,49059,43265,43439,35358,49675,48139,43149,47824,47724,43722,48503,48867,43106,48409,47876,48090,47766,48210,49052,47771,48269,43441,48407,35332,43178,49830,43228,48868,47736,35349,48947,47910,43535,47997,47916,48890,43071,43490,43464,47914,49286,43231,47875,35377,43465,49251,47822,43507,47772,43191,47685,47826,49828,49352,43571,48331,43458,47825,43131,47823,48920,48922,43492,47872,43229,47803,47792,47774,43282,48207,48915,43115,43396,48805,48596,43431,47785,43437,49047,48191,43522,43515,48618,49096,47921,48384,49112,35359,48408,43209,48179,48277,49152,43177,35373,43570,47897,47719,48240,49184,47770,48691,47763,49674,47885,48427,48559,47773,48180,48239,43070,35328,35355,35320,48921,49095,48060,47732,49829,49304,48062,43284,35379,47891,49709,43432,43494,35350,43102,43726,47668,43161,35329,49011]

positive_users = [49581,48049,48438,49538,48517,43558,43319,48532,48588,43497,43737,49078,43226,49353,48386,43197,43321,47730,35363,43361,47711,48253,43223,48005,48415,81359,43413,43149,49746,48064,47663,48007,43460,48300,48029,43409,48389,47990,43106,43547,49613,49368,47810,48716,47690,43244,35344,49369,43705,35332,48066,43568,43693,43620,47820,35349,48429,47899,48385,48694,47913,48154,43190,43123,49020,49647,35370,48512,49323,48574,49823,48452,47718,48699,47920,35377,43465,47807,43330,48578,48223,49875,47826,48714,47825,43131,43492,47670,43229,43307,48329,49838,49688,48178,48915,43396,43066,49640,48432,43305,49370,48981,49324,48522,48775,48035,47701,49539,47936,48776,47919,48980,48418,47912,47854,47740,49231,47911,49837,49237,49731,43235,47692,48953,49541,35328,49610,47974,49092,43420,48552,48388,43284,48505,48721,48816,47710,49745,43432,43494,43312,48055,48509,47708,48494,43493,49580,43550,48850,47713,47668,47752,47746,48580,49326,43161,47676,49612,43264,43393,48684,47980,49235,35323,47915,48321,43706,43185,49732,48387,35331,47888,43643,49836,48683,35324,47782,48510,43451,48754,47724,43120,47687,48440,35362,47748,49079,48269,43735,49232,49294,48126,49016,43662,48458,49611,47866,49206,49367,43301,47890,49330,49638,43231,49687,35354,48243,49488,43585,49105,48715,43191,47691,49327,48435,49325,48713,47981,48807,47652,48598,47712,43418,43067,49238,49600,43557,43488,48979,48439,49423,48098,43320,43549,48571,49320,43537,43556,43227,47790,43278,49152,38563,43570,43698,48436,48437,49272,43148,48441,49648,43632,48763,43128,47885,48813,43629,48054,43681,38199,43257,48817,48511,43070,43468,43739,49077,47709,35379,48448,49361,43626,48904,49287,49500,38148,35322,48383,43450,48493,49730,43173,43729,47908]
negative_users = []
for user in positive_users:
    
    user_neighbors = list(net.neighbors(user))

    neg = list(set(positive_users) ^ set(user_neighbors))
    negative_users.extend(neg[0:6])


# user id, feature 1, feature 3, class label

#create postive dataset
data = []

for p in positive_users:
    ActiveUsers = (get_f1(p))
    AiNoAN = get_f3(ActiveUsers)
    data.append([p, len(ActiveUsers), int(AiNoAN), 1])

#create negative dataset

for n in negative_users:
    ActiveUsers = (get_f1(p))
    AiNoAN = get_f3(ActiveUsers)
    data.append([p, len(ActiveUsers), int(AiNoAN), 0])

  
df = pd.DataFrame(data, columns=['user_id', 'F1', 'F3', 'Class'])
df.to_csv('dataset.csv',header=True,index=False)



dataSet = pd.read_csv('dataset.csv')
Y = dataSet.pop('Class') # Class
X = dataSet.drop('user_id',axis = 1)
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 60)
# trainall(X_train, X_test, Y_train, Y_test)


#Improvement: More features(ex.PNE), more users, more topics, imbalanced dataset/more realistic.

#Questions for Marin: What is PNE? Need access to database with forum 77! How to get root neighbor for negative samples?


# Creating the Model (Optimised)
# model = RandomForestClassifier(max_depth=2, random_state=0)
# model.fit(X_train,Y_train)
# Y_pred = model.predict(X_test)
# conf = round((r2_score(Y_test,Y_pred))*100,3)

# # Printing Confidence of Our Model
# print('Model Confidence : ' , conf)
# print('Confusion Matrix : \n' , confusion_matrix(Y_test, Y_pred))